{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d349f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe2e9f8-44c2-43d8-b207-be9ba282a101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T21:41:06.127679Z",
     "iopub.status.busy": "2023-02-14T21:41:06.127294Z",
     "iopub.status.idle": "2023-02-14T21:41:26.195738Z",
     "shell.execute_reply": "2023-02-14T21:41:26.193990Z",
     "shell.execute_reply.started": "2023-02-14T21:41:06.127651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus.jensen@vg.no/opt/anaconda3/envs/eventyr/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"flax-community/norsk-gpt-wiki\" # replace with your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6daf3951-643c-4d56-8ca6-625016fac64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T21:42:34.669467Z",
     "iopub.status.busy": "2023-02-14T21:42:34.668615Z",
     "iopub.status.idle": "2023-02-14T21:42:34.815240Z",
     "shell.execute_reply": "2023-02-14T21:42:34.813796Z",
     "shell.execute_reply.started": "2023-02-14T21:42:34.669439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnus.jensen@vg.no/opt/anaconda3/envs/eventyr/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "         tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, data_collator\n",
    "\n",
    "train_path = \"eventyr_train.txt\" # replace with your training file\n",
    "train_dataset, data_collator = load_dataset(train_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1fd7c7-f46e-4df1-9587-f356e7f855b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T21:42:36.891975Z",
     "iopub.status.busy": "2023-02-14T21:42:36.891623Z",
     "iopub.status.idle": "2023-02-14T21:42:38.924627Z",
     "shell.execute_reply": "2023-02-14T21:42:38.923049Z",
     "shell.execute_reply.started": "2023-02-14T21:42:36.891947Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/outputs/model\", # The output directory\n",
    "    overwrite_output_dir=True, # Overwrite the content of the output directory\n",
    "    num_train_epochs=300, # Number of training epochs\n",
    "    per_device_train_batch_size=8, # Batch size for training\n",
    "    save_steps=1000, # After # steps model is saved\n",
    "    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n",
    "    fp16=True # Activate float-point=16 precision to train faster\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fce402-078c-47f2-9395-2c4730ca4766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T21:42:42.183390Z",
     "iopub.status.busy": "2023-02-14T21:42:42.182325Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=False)\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eventyr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "309212cfb2d1f9c7c552c6c44acfd7ae4869399c140b1bf3010819b105e87330"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
